{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS02 - Data Science with Python \n",
    "\n",
    "## Introduction to Pandas\n",
    "\n",
    "Welcome to our second workshop in the ***Data Science with Python*** series! \n",
    "\n",
    "In this workshop we will introduce our second Python package Pandas. This is an extremely useful package four datascience as its main focus in on storing and operating on data with structures called dataframes. It if a widely used package in most data science workflow due to the ease of use of the data structure, along with the many in built methods that are part of the package that make working with data in Python so much easier. \n",
    "\n",
    "> **Author**: Tania Turdean, Science Executive (19/20), UCL Data Science Society (tania.turdean.19@ucl.ac.uk)\n",
    ">\n",
    "> **Updated by**: Philip Wilkinson, Head of Science (20/21), UCL Data Science Society (philip.wilkinson.19@ucl.ac.uk)\n",
    ">\n",
    "> Proudly presented by the UCL Data Science Society\n",
    "\n",
    "In this workshop we will cover: \n",
    "\n",
    "## 1) [What is Pandas?](#wip)\n",
    "\n",
    "## 2) [Pandas Series](#ps)\n",
    "\n",
    "## 3) [Pandas DataFrame](#pd)\n",
    "\n",
    "## 4) [Accessing data](#ad)\n",
    "\n",
    "## 5) [DataFrame operations](#do)\n",
    "\n",
    "Which should set the basics allowing you to use Pandas in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"wip\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Pandas? \n",
    "\n",
    "Pandas is an open source Python package that is widely used for data science/analysis and machine learning tasks, building on top of the Numpy package that we introduced last week. It is one of the most popular data wrangling packages for data science workflows and integrates well with many of the other libraries used during this process. It is useful because it makes tasks like:\n",
    "\n",
    "- Data cleaning\n",
    "- Data filling\n",
    "- Data normalisation\n",
    "- Merges and joins\n",
    "- Data visualisation\n",
    "- Data inspection\n",
    "\n",
    "so much simpler than they would otherwise be. This functionality helps with its aim of becoming:\n",
    "\n",
    "> ...the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language.\n",
    "\n",
    "as described in the [documentation](https://pandas.pydata.org/about/).\n",
    "\n",
    "With this in mind then, what can pandas be used for and what can we do with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing pandas\n",
    "\n",
    "If you have not installed pandas on your system you will want to uncomment the following code and run it (only run one of these):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install pandas\n",
    "#!conda install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Importing Pandas\n",
    "\n",
    "In order to be able to use pandas we need to import it, just like we did with numpy previously. Typically it is imported as `pd` in order to save writing `pandas` everytime we need to use it so we will do just that here. We will also import Numpy again just in case we need it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Importing Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Series\n",
    "\n",
    "Pandas series are the fundamental struture that make up the building blocks of Pandas dataframe. They are similar to Numpy arrays but they are different in that we can specify an index for a pandas series that is always linked, allowing us to access the location of the information using the index. In most other respects, such as when applying numerical operations, series can behave much like arrays. \n",
    "\n",
    "To generate a series, we need to use `pd.series(x)` where `x` is a lit to be converted to a series. For this, we can also pass the argument `index=y` where `y` is a list of the indices we want to use for the respctive series values, meaning that we can use both numerical and string notation (i.e. min, median, max rather than 1,2,3 if we so desired. \n",
    "\n",
    "Below are two code cells, the first of which passes no `index` argumenent, meaning that by default the indexes will be as we expect for a list and/or a python array (i.e. integers starting from $0$). The second cell creates a series but passes another list for the `index` argument, showing us how we can use a list of strings rather than the standard notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Pandas series\n",
    "series = pd.Series([10,20,30,40])\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Pandas series\n",
    "series = pd.Series([10,20,30,40], index=[\"a\",\"b\",\"c\",\"d\"])\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"pd\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas DataFrame\n",
    "\n",
    "A data frame is essentially a table of data stored as a variable similar to how you store multiple values in an array as a variable. Note the two are **not** the same! Each column in a data frame is represented by a Pandas series. \n",
    "\n",
    "We can create a DataFrame in two ways, either by converting data already stored in Python into a Dataframe or by reading in an extrnal file. In the first instance, we can create a dataframe from a variety of structures, such as from lists or a list of lists or from zipped lists. In our case however we will use a dictionary of lists as follows using the `pd.DataFrame()` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary of lists\n",
    "data_dict = {\"Countries\": [\"England\", \"Scotland\", \"Wales\", \"Northern Ireland\"],\n",
    "            \"Capitals\": [\"London\", \"Edinburgh\", \"Cardiff\", \"Dublin\"],\n",
    "            \"Population (millions)\": [55.98,  5.454, 3.136, 1.786]}\n",
    "\n",
    "#convert this to a dataframe\n",
    "data_df = pd.DataFrame(data_dict)\n",
    "\n",
    "#display the dataframe\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other examples of using generated data to create a DataFrame can include passing lists. Here, Unlike with dictionaries, this approach creates the rows with the column nmes to be specified afterwards. Here we can pass a list containing a number of lists, with each list within the greater list determining the rows. The columns are then specified by the second argument `columns=[\"column a\", column b\"]` where the input is a list of strings that determine the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a set of lists\n",
    "x = [\"a1\", \"b1\"]\n",
    "y = [\"a2\", \"b2\"]\n",
    "\n",
    "# Combines the two lists into a list containing the lists x and y\n",
    "z = [x,y]\n",
    "\n",
    "# Generates a data frame from the lists x and y\n",
    "df = pd.DataFrame(z,columns=[\"column a\", \"column b\"])\n",
    "\n",
    "# Displays data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can extract the data from an existing datasource, such as a **csv (comma seperated variables)** file using `pd.read_csv`. Inside the file each column is seperated by a comma as the name suggests. The file is read and the data converted to a data frame which is then displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines df as a data frame without adding an additional index column \n",
    "df = pd.read_csv(\"StarColours.csv\")\n",
    "\n",
    "# Displays the data frame df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data within the file has been converted into a table. Notice the additional column added to the left of the data frame. This is the index column essentially providing a unique identifier for each row in the table. We can extract the data without this column by specifiying: \n",
    "\n",
    "````df = pd.read_csv(\"StarColours.csv\", index_col=0) ````\n",
    "\n",
    "which tells essentially sets the index column to be coloumn zero (once again Python starts counting from $0$). This is shown in the code cell below. Note that here the index column isn't entirely unique and often its preferable for the index column to consist of unique identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines df as a data frame without adding an additional index column\n",
    "df = pd.read_csv(\"StarColours.csv\", index_col=0)\n",
    "\n",
    "# Displays the data frame df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delimiter\n",
    "The function ````pd.read_csv()```` assumes by default that your variables are seperated with commas. This is not always the case however. In such cases you'll require a delimiter argument and also potentially an engine argument. The delimiter argument specifies the seperation between columns. By default this is set to a comma. However, the file to be read below contains columns seperated by three spaces. We pass the argument ````delimiter=\"   \"```` to account for the way in which the columns are seperated. This leads to another problem and that is that Pandas is not written entirely in Python. Pandas also makes use of the C engine which doesn't recognise seperators greater than one character. This is fixed by specifying ````engine=\"python\"```` so that Python reads the seperator instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverts data within the specified file into a data frame using the Python engine\n",
    "df = pd.read_csv(\"H2O.csv\", delimiter=\"   \",engine=\"python\")\n",
    "\n",
    "# Displays the data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to being able to read dataframes from files, Pandas can also convert tables on websites into dataframes using `pd.read_html`. This requires you to pass the URL containing the desired table as a string and a match argument as well. Here passing `match = \"keyword\"` means that pandas will try to convert only the tables wiyth the `\"keyword\"` string into a DataFrame.\n",
    "\n",
    "In the code below we attempt to convert a table of conductivities from [wikipedia](https://en.wikipedia.org/wiki/Electrical_resistivity_and_conductivity). Here theare a are multiple tabls so we pass the `match=\"coefficient\"` argument. For this, it is important to note that the output for this function is a list of DataFrames not a single DataFrame. Here, to display the data the line `df = df[0]` was needed before it could be displayed. Another problem is that some websites block this feature so you won't be able to use this function to convert those tables into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts conductivity table from Wikipedia into a data frame\n",
    "df = pd.read_html(\"https://en.wikipedia.org/wiki/Electrical_resistivity_and_conductivity\",match=\"coefficient\")\n",
    "\n",
    "# Displays data frame\n",
    "df = df[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to do the same with the first table from the link where you can use the word `Plasma` as a match. How can you read this in differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ad\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing data \n",
    "\n",
    "### DataFrame Slicing\n",
    "\n",
    "Just as with a list, an array or dictionary we can also take a look at specific values, or ranges of values, from with the datastructure. In the case of a DataFrame we can look at slices just as we would with a list or an array, but we can use the column names or the indexes to access this information. \n",
    "\n",
    "Returning to the original data frame we will look at how to select specific columns in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines df as a data frame without adding an additional index column \n",
    "df = pd.read_csv(\"StarColours.csv\")\n",
    "\n",
    "# Displays the data frame df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select a specific column in a data frame we slice in a similar fashion to how we would select a dictionary value. The key here would be the column name as a string. Doing this returns the selected column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the colour column\n",
    "df[\"Color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can slice further to select a specfic item or set of item in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects seventh item in colour column\n",
    "df[\"Color\"][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects 4th to 6th items in colour column\n",
    "df[\"Color\"][3:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also select multiple columns. Note the need for another set of square brackets as the argument passed is a list of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the specified columns\n",
    "df[[\"Color\", \"Main Characteristics\", \"Examples\"]][3:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.loc\n",
    "\n",
    "Beyond slicing to find information, we can also select certain rows based on specified column conditions using `df.loc`. This is where we can begin to use conditional statements to access information from a dataset. For example, `df.loc[df[\"columnname\"] > x]` will return all rows with a value greater than `x` for the column labelled `columnname`. Equally, we could also look for rows with values equal to a particular value, or using other comparison operators mentioned in the logic workshop.\n",
    "\n",
    "Examples of this include finding rows with values greater than $1$ in the \"Average Mass (The Sun =1)\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all rows with values greater than \n",
    "#one for the average mass column\n",
    "df.loc[df[\"Average Mass (The Sun = 1)\"] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same operation but extracting only the examples column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all rows with values greater than \n",
    "#one for the average mass column\n",
    "df.loc[df[\"Average Mass (The Sun = 1)\"] > 1, \"Examples\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or finding all rows where the \"Rigal Spice\" in the examples column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all rows with \"Rigel  Spica\" in the \"Examples\" column\n",
    "df.loc[df[\"Examples\"] == \"Rigel  Spica\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we can use this assignment to change the value of a given cell if we use assignment at the end of finding the specific row. For example, try changing the color of the row with \"10 Lacertra\" as an example to \"Violet\", checking the row, and then changing the value back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"do\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame operations\n",
    "\n",
    "The format of the DataFrame allows much in built functionality, including the ability to perform mathematical operations on columns and across them, the creation of new columns to add to datasets and a load of inbuilt functionality as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathmatical Operations on Data Frame Columns\n",
    "\n",
    "Performing mathmatical operations on data frame columns means the operation will be performed for each element in equivalent positions in the column just as it would for a Numpy array which we covered last workshop.\n",
    "\n",
    "For example, we can add the mass averge and the mass radius columns together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the average mass and radius columns\n",
    "df[\"Average Mass (The Sun = 1)\"] + df[\"Average Radius (The Sun = 1)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can multiple the mas by three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Average Mass (The Sun = 1)\"] * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining New Columns\n",
    "\n",
    "Once an operation has been performed on a column, we can assign the results back to the original dataframe by slicing for a column that doesn't yet exist (or if we wanted to overwrite the original column we could specify that column). Here, we will define a density column from the average mass and average radius column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a new average density column accordingly\n",
    "df[\"Average Density\"] = ((df[\"Average Mass (The Sun = 1)\"]\n",
    "                          *3)\n",
    "                         /(4*\n",
    "                           np.pi*\n",
    "                           (df[\"Average Radius (The Sun = 1)\"]\n",
    "                            **3)))\n",
    "\n",
    "# Displays the data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame Functions\n",
    "\n",
    "Most functions within Pandas spefically apply to data frames and are called by typing `df.function_name()`. Here `df` is the data frame and `function()` is the function applied to the data frame. Often you can apply these to column slices by typing `df[\"column name\"].function()`. Examples of these include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df.head()\n",
    "This returns the first set of rows in the data frame. By default this is the first five rows. Alternatively you can specify the a number of rows to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns first five rows from the data frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns first two rows from data frame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df.tail() \n",
    "Identical to ````df.head()```` except that it the last set of rows is returned instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns last five rows from the data frame\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns last two rows from data frame\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df.unique()\n",
    "Returns unique items as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns unique items from the colours column\n",
    "df[\"Color\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df.describe()\n",
    "Returns a number of statistical parameters for all colums with numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns unique items from the colours row\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you only wanted one of the rows above you could choose only to find the specific row instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the mean of each numerical column\n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df.set_index\n",
    "If I already have a data frame and want to select an index then I can use this function. I pass the column name as the argument. Additionally, writing ````inplace=True```` as an argument means the data frame will be redefined with this new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Replaces the index column for df with the \"Color\" column\n",
    "df.set_index(\"Color\", inplace=True)\n",
    "\n",
    "# Displays data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Columns to Arrays, Lists and Dictionaries\n",
    "\n",
    "Using ````df.values````,````df.tolist()```` and ````df.to_dict()```` columns can be converted into arrays, lists and dictionaries respectively. When converting to a dictionary the index column provides the key for each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Converts the column to an array\n",
    "df[\"Examples\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts column to a list\n",
    "df[\"Examples\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts column to a dictionary\n",
    "df[\"Examples\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping a Row or Column\n",
    "Using ````df.drop```` it is possible to drop a row or column from a data frame. The first argument requires the row index or column name. This can instead be a list of indexes or column names if you want to drop multiple at the same time. The second argument determines whether you are removing a row or column. Using ````axis=0```` will drop the row with the specified index while ````axis=1```` will drop the specified column. Writing ````inplace=True```` redefines your data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops the row with \"Red\" as its index\n",
    "df.drop(\"Red\",axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops rows with \"Red\" and \"Blue\" as their index\n",
    "df.drop([\"Red\",\"Blue\"],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drops the \"Average Density\" column\n",
    "df.drop(\"Average Density\",axis=1,inplace=True)\n",
    "\n",
    "# Displays data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Functions\n",
    "With Pandas like its possible to apply these functions one after the other. A generic example would look something like ````df.function1().function2()```` where ````function1()```` is the first function and ````function2()```` is the second function. Here we will drop two columns at once using the drop function twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drops the main characteristics and examples column\n",
    "df.drop(\"Main Characteristics\",axis=1).drop(\"Examples\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Dataframes\n",
    "Using ````df1.append(df2)```` you can merge two similar data frames together. Provided they have similar columns,\n",
    "this will attach ````df2```` to the bottom of ````df1````. Here we append a data frame to itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins the data frame df with itself\n",
    "df.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN Variables\n",
    "\n",
    "You may have noticed that in some of our DataFrames a few NaN values appear. These stand for \"Not a Number\" and are invalid entries which you'll likely want to eliminate when performing any data analysis. To handle this we can use functions such as:\n",
    "\n",
    "### df.dropna()\n",
    "\n",
    "A brute force way of dealing with NaN variables is to use this function. This essentially removes all rows containing a NaN variable. Passing ````inplace=True```` as an argument redefines the data frame as one with all rows containing NaN removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts conductivity table from Wikipedia into a data frame\n",
    "df = pd.read_html(\"https://en.wikipedia.org/wiki/Electrical_resistivity_and_conductivity\",\n",
    "                  match=\"coefficient\")\n",
    "\n",
    "# Displays data frame\n",
    "df = df[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops all rows containing NaN variables.\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, passing ````axis=1```` will eliminate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops all columns containing NaN variables.\n",
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df.fillna\n",
    "When we used ````df.dropna()```` above all rows with NaN data were removed. This can be a problem because we are losing information from the other columns that don't have NaN data. A way of dealing with this is to use ````df.fillna(x)```` instead. Here, instead of removing rows we replace any NaN with whatever ````x```` is. Typically this would be chosen to be some extreme value that would be recognised by an algorithm as an outlier. Again the argument ````inplace=True```` can be used to redefine the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces NaN variables with -99999999\n",
    "df.fillna(-99999999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data Frames to Files\n",
    "Just as we can read a file and turn the data within into a data frame, we can also convert data within data frames to files. \n",
    "#### df.to_csv \n",
    "To generate a csv file from a data frame simple write the line ````df.to_csv(\"filename.csv\")```` where ````df```` is the variable the data frame is defined as and ````\"filename.csv\"```` is the name of the csv file that will be generated. We often pass an additional argument ````index=False````. This removes the index column when converting to the file. Often this column will be unesserary and meaningless, only serving to take up space. When reading the file an index column is typically generated anyway so storing the index column isn't necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts data frame into a csv file without an index column\n",
    "df.to_csv(\"conductivities.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Types of File\n",
    "Other commands can produce other types of files from data frames. One example of this is ````df.to_excel(\"filename.xlsx\")```` which creates an Excel file with the name ````filename.xlsx````. One can equally create a data frame from such a file. The equivalent command for the Excel file for this is ````pd.read_excel(\"filename.xlsx\")````."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1) StarColours.csv file data obtained from: https://www.enchantedlearning.com/subjects/astronomy/stars/startypes.shtml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
